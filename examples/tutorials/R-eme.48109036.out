Running in parallel on 20 cores
current state: start
  0%|          | 0/37 [00:00<?, ?it/s]100%|██████████| 37/37 [00:00<00:00, 4185.25it/s]
current state: mode_solving
div 1 1
div 2 1
div 3 1
div 4 1
div 5 1
div 6 1
div 7 1
div 8 1
div 0 1
total 0
total 0
total 0
total 0
total 0
total 0
ser 0 108563286 5868968
ser 0 108563286 5868963
ser 0 108563286 5868798
ser 0 108563286 5868758
ser 0 108563286 5868598
ser 0 108563286 5868543
ser 0 108563286 5868398
ser 0 108563286 5868373
ser 0 108563286 5868303
ser 0 108563286 5868178
ser 0 108563286 5868044
ser 0 108563286 5867949
ser 0 108563286 5867859
ser 0 108563286 5867774
ser 0 108563286 5867619
ser 0 108563286 5867584
ser 0 108563286 5867321
ser 0 108563286 2934527
ser 0 108563286 2934392
ser 0 108563286 2934337
total 108563286
div 18 1
total 0
div 15 1
div 19 1
div 16 1
total 0
total 0
div 17 1
total 0
total 0
div 9 1
total 0
div 10 1
total 0
div 11 1
total 0
div 12 1
total 0
div 13 1
total 0
div 14 1
total 0
total 0
total 0
0it [00:00, ?it/s]1it [00:22, 22.98s/it]2it [01:08, 36.14s/it]2it [01:08, 34.17s/it]
current state: finished_modes
current state: layer_propagating
div 5 9
total 0
div 6 9
total 0
div 1 9
total 0
div 2 9
total 0
div 3 9
total 0
div 4 9
total 0
div 0 9
ser 0 4355817060 187921599
ser 0 4355817060 187660109
ser 0 4355817060 187631744
ser 0 4355817060 194830212
ser 0 4355817060 202040503
ser 0 4355817060 209250065
ser 0 4355817060 216451916
ser 0 4355817060 216540171
ser 0 4355817060 231183191
ser 0 4355817060 245774638
ser 0 4355817060 260310166
ser 0 4355817060 282098804
ser 0 4355817060 289184612
ser 0 4355817060 288974172
ser 0 4355817060 289099442
ser 0 4355817060 289025872
ser 0 4355817060 144245531
ser 0 4355817060 144450011
ser 0 4355817060 144522521
ser 0 4355817060 144621781
total 4355817060
div 18 9
total 0
div 16 9
div 19 9
total 0
div 17 9
div 8 9
div 15 9
total 0
div 14 9
total 0
div 9 9
total 0
div 10 9
div 11 9
div 12 9
div 13 9
div 7 9
total 0
total 0
total 0
total 0
total 0
total 0
total 0
total 0
Traceback (most recent call last):
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/fslhome/ihammond/.local/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    main()
  File "/fslhome/ihammond/.local/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    run_command_line(args)
  File "/fslhome/ihammond/.local/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_path(sys.argv[0], run_name='__main__')
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 268, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "adiabatic.py", line 79, in <module>
    eme.propagate()
  File "/zhome/ihammond/GitHub/emepy/emepy/eme.py", line 599, in propagate
    self.propagate_layers()
  File "/zhome/ihammond/GitHub/emepy/emepy/eme.py", line 212, in propagate_layers
    results = self._run_parallel_functions(*tasks)
  File "/zhome/ihammond/GitHub/emepy/emepy/eme.py", line 699, in _run_parallel_functions
    new_data = []
  File "/zhome/ihammond/GitHub/emepy/emepy/eme.py", line 639, in batch_scatter
    
  File "mpi4py/MPI/Comm.pyx", line 1587, in mpi4py.MPI.Comm.scatter
  File "mpi4py/MPI/msgpickle.pxi", line 823, in mpi4py.MPI.PyMPI_scatter
  File "mpi4py/MPI/msgpickle.pxi", line 167, in mpi4py.MPI.pickle_dumpv
  File "mpi4py/MPI/msgbuffer.pxi", line 50, in mpi4py.MPI.downcast
OverflowError: integer 2339594413 does not fit in 'int'
