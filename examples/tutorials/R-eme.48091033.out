Running in parallel on 50 cores
current state: start
  0%|          | 0/37 [00:00<?, ?it/s]100%|██████████| 37/37 [00:00<00:00, 4294.95it/s]
current state: mode_solving
0it [00:00, ?it/s]1it [00:15, 15.50s/it]1it [00:15, 15.50s/it]
current state: finished_modes
current state: layer_propagating
Traceback (most recent call last):
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/fslhome/ihammond/.local/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    main()
  File "/fslhome/ihammond/.local/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    run_command_line(args)
  File "/fslhome/ihammond/.local/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_path(sys.argv[0], run_name='__main__')
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 268, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/fslhome/ihammond/miniconda3/envs/emepy/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "adiabatic.py", line 79, in <module>
    eme.propagate()
  File "/zhome/ihammond/GitHub/emepy/emepy/eme.py", line 598, in propagate
    self.propagate_layers()
  File "/zhome/ihammond/GitHub/emepy/emepy/eme.py", line 211, in propagate_layers
    results = self._run_parallel_functions(*tasks)
  File "/zhome/ihammond/GitHub/emepy/emepy/eme.py", line 658, in _run_parallel_functions
    
  File "/zhome/ihammond/GitHub/emepy/emepy/eme.py", line 612, in batch_scatter
    
  File "mpi4py/MPI/Comm.pyx", line 1587, in mpi4py.MPI.Comm.scatter
  File "mpi4py/MPI/msgpickle.pxi", line 823, in mpi4py.MPI.PyMPI_scatter
  File "mpi4py/MPI/msgpickle.pxi", line 167, in mpi4py.MPI.pickle_dumpv
  File "mpi4py/MPI/msgbuffer.pxi", line 50, in mpi4py.MPI.downcast
OverflowError: integer 2247459711 does not fit in 'int'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD 
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
