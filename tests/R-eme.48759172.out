Running in parallel on 20 cores
*** TESTING GRADIENT ADJOINT ***
Now performing an optimization...

step size: 1e-10
Adjoint FOM: 1.9521019707102925
Adjoint gradient: [  1759.97286517   4160.94741423 -17338.68420651]

FD FOM: 1.9502891762814347
FD gradient: [-628.8358622708046, 2984.544656969845, -1006.9566647175066]

Now performing an optimization...

step size: 1e-09
Adjoint FOM: 1.9521018869753226
Adjoint gradient: [-1912.82085108  -216.86686838 17997.65571255]

FD FOM: 1.9502881846063662
FD gradient: [-776.5959202377104, 138.6614438825262, 601.3854509934191]

Now performing an optimization...

step size: 1e-08
Adjoint FOM: 1.952102191872854
Adjoint gradient: [-20875.69610209  24798.15860742  -5982.24629362]

FD FOM: 1.950288436223102
FD gradient: [-12.55043767800501, -0.7407253477254017, -17.0450981218373]

Now performing an optimization...

step size: 1e-07
Adjoint FOM: 1.9521017711263466
Adjoint gradient: [ -3349.75990389  17997.59634568 -10379.58504971]

FD FOM: 1.9502891495811099
FD gradient: [-2.2934325361045893, 0.510020996546956, 2.53720677689806]

Now performing an optimization...

step size: 1e-06
Adjoint FOM: 1.9521019121929268
Adjoint gradient: [  7038.77636647 -13290.1049034  -21880.16209342]

FD FOM: 1.9502881872448392
FD gradient: [-0.18358587272615523, 0.23421446149196612, -0.19480088220280578]

Now performing an optimization...

step size: 1e-05
Adjoint FOM: 1.9521018216267612
Adjoint gradient: [ 26192.27096366 -13956.45129835 -14136.55205596]

FD FOM: 1.9502874934860792
FD gradient: [0.13742695075258382, 0.008274646978723155, -0.13526629076832108]

Now performing an optimization...

step size: 0.0001
Adjoint FOM: 1.9521020467887307
Adjoint gradient: [   966.47063613  -2120.78618604 -10139.87870358]

FD FOM: 1.95028149935785
FD gradient: [0.004852375816710719, -0.036911858065646896, 0.0022959571710501336]

Now performing an optimization...

step size: 0.001
Adjoint FOM: 1.9521019925462986
Adjoint gradient: [10321.52057874  1657.78632059 13161.7445932 ]

FD FOM: 1.9502044075148806
FD gradient: [0.022448787539097736, 0.010892138958551456, -0.04802030760409082]

Now performing an optimization...
..
----------------------------------------------------------------------
Ran 2 tests in 7036.732s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.700s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.699s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.702s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.710s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.698s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.704s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.714s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.714s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.758s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.729s

OK
........
----------------------------------------------------------------------
Ran 2 tests in 7036.583s

OK
.
----------------------------------------------------------------------
Ran 2 tests in 7036.565s


----------------------------------------------------------------------
Ran 2 tests in 7036.552s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.565s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 7036.553s

OK
.OK

----------------------------------------------------------------------
Ran 2 tests in 7036.580s

OK
.
----------------------------------------------------------------------
Ran 2 tests in 7036.555s

OK
.
----------------------------------------------------------------------
Ran 2 tests in 7036.591s

OK

step size: 0.01
Adjoint FOM: 1.952101445749983
Adjoint gradient: [  4073.49354192  19762.35272959 -13119.11213759]

FD FOM: 1.948198074961808
FD gradient: [-0.005465241485602679, -0.04418761930062853, -0.04234998355001629]

..
----------------------------------------------------------------------
Ran 2 tests in 7036.731s

OK
