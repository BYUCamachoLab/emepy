Running in parallel on 20 cores
*** TESTING GRADIENT ADJOINT ***
Now performing an optimization...
start 0.2558612214530483
end 1.9502891548276153 1.950289072388228 1e-09 7 0.25586122145304824

step size: 1e-09
Adjoint FOM: 1.9521019707102925
Adjoint gradient: [1759.97039417]

FD FOM: 1.9502891136079217
FD gradient: [41.21969365833422]

Now performing an optimization...
start 0.25586123045304826
end 1.9502888315302114 1.9502889550632925 1e-08 7 0.25586123045304826

step size: 1e-08
Adjoint FOM: 1.9521020840309693
Adjoint gradient: [1759.96924888]

FD FOM: 1.950288893296752
FD gradient: [-6.176654054890207]

Now performing an optimization...
start 0.25586132045304827
end 1.9502890626771776 1.9502889085681898 1e-07 7 0.25586132045304827

step size: 1e-07
Adjoint FOM: 1.9521017250032267
Adjoint gradient: [1759.95929662]

FD FOM: 1.9502889856226837
FD gradient: [0.7705449389661112]

Now performing an optimization...
start 0.25586222045304824
end 1.9502890267658377 1.9502886870538911 1e-06 7 0.2558622204530483

step size: 1e-06
Adjoint FOM: 1.9521019891559515
Adjoint gradient: [1759.87182214]

FD FOM: 1.9502888569098644
FD gradient: [0.169855973286559]

Now performing an optimization...
start 0.2558712204530483
end 1.9502878497267921 1.950287627283254 1e-05 7 0.2558712204530483

step size: 1e-05
Adjoint FOM: 1.9521016078609443
Adjoint gradient: [1758.95297874]

FD FOM: 1.9502877385050232
FD gradient: [0.011122176901690038]

Now performing an optimization...
start 0.25596122045304825
end 1.9502822589110607 1.9502806081371786 0.0001 7 0.25596122045304825

step size: 0.0001
Adjoint FOM: 1.9521007936356338
Adjoint gradient: [1749.80571007]

FD FOM: 1.9502814335241196
FD gradient: [0.00825386941016859]

Now performing an optimization...
start 0.25686122045304827
end 1.950216402248403 1.9501927684529794 0.001 7 0.25686122045304827

step size: 0.001
Adjoint FOM: 1.9521013937591878
Adjoint gradient: [1657.68184661]

FD FOM: 1.9502045853506913
FD gradient: [0.011816897711791619]

Now performing an optimization...
start 0.2658612204530483
..
----------------------------------------------------------------------
Ran 2 tests in 3031.223s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 3031.429s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 3031.210s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 3031.217s

OK
...
----------------------------------------------------------------------
Ran 2 tests in 3031.213s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 3031.198s

OK
.
----------------------------------------------------------------------
..
----------------------------------------------------------------------
Ran 2 tests in 3030.681s

OK
Ran 2 tests in 3031.198s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 3030.673s

OK
...
----------------------------------------------------------------------
Ran 2 tests in 3030.657s

OK
.
----------------------------------------------------------------------
Ran 2 tests in 3030.674s

OK
.....
----------------------------------------------------------------------

----------------------------------------------------------------------
Ran 2 tests in 3030.661s

OK
Ran 2 tests in 3031.198s

OK
....
----------------------------------------------------------------------
Ran 2 tests in 3030.652s

OK

----------------------------------------------------------------------
Ran 2 tests in 3030.671s

..OK

----------------------------------------------------------------------
Ran 2 tests in 3030.890s

OK
.
----------------------------------------------------------------------
Ran 2 tests in 3030.687s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 3031.232s

OK
..
----------------------------------------------------------------------
Ran 2 tests in 3030.652s

OK
end 1.9482768571707338 1.9481123041699726 0.01 7 0.2658612204530483

step size: 0.01
Adjoint FOM: 1.9521017711263466
Adjoint gradient: [762.96758739]

FD FOM: 1.9481945806703531
FD gradient: [0.008227650038061807]

..
----------------------------------------------------------------------
Ran 2 tests in 3030.657s

OK
